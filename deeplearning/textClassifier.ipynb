{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 读取所有文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_technology = pd.read_csv('../data/technology_news.csv',encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_tour = pd.read_csv('../data/tour.csv',encoding='gbk')\n",
    "df_tour = df_tour.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv('../data/entertainment_news.csv',encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv('../data/military_news.csv',encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv('../data/sports_news.csv',encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:6000]\n",
    "tour = df_tour.content.values.tolist()[1000:6000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:5000]\n",
    "military = df_military.content.values.tolist()[:5000]\n",
    "sports = df_sports.content.values.tolist()[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 读取预测的单一文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Mars\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.888 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "ori_data = pd.read_csv('C:/Users/Mars/Desktop/july/pachong/data/data.txt',index_col=False,names = ['content'])\n",
    "ori_data = ori_data.dropna()\n",
    "# ori_data.head(10)\n",
    "content = ori_data.content.values.tolist()\n",
    "segment = []\n",
    "for line in content:\n",
    "    try:\n",
    "        segs = jieba.lcut(line)\n",
    "        for seg in segs:\n",
    "            if len(seg)>1 and seg!='\\r\\n':\n",
    "                segment.append(seg)\n",
    "    except:\n",
    "        print(line)\n",
    "        continue\n",
    "# print(segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 读取停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('../data/stopwords.txt',index_col=False,quoting=3,sep='\\t',\n",
    "                        names=['stopword'],encoding='utf-8')\n",
    "stopwords = stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 定义数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess ending!\n"
     ]
    }
   ],
   "source": [
    "def preprocess(content_line,sentences,category):\n",
    "    for line in content_line:\n",
    "        try:\n",
    "            segs = jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1,segs)\n",
    "            segs = filter(lambda x:x not in stopwords,segs)\n",
    "            sentences.append((\" \".join(segs),category))\n",
    "        except:\n",
    "            print(line)\n",
    "            continue\n",
    "            \n",
    "sentences = []\n",
    "preprocess(technology,sentences,'tenchnology')\n",
    "preprocess(tour,sentences,'tour')\n",
    "preprocess(entertainment,sentences,'entertainment')\n",
    "preprocess(military,sentences,'military')\n",
    "preprocess(sports,sentences,'sports')\n",
    "\n",
    "print(\"preprocess ending!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segs = []\n",
    "segment = filter(lambda x:x not in stopwords,segment)\n",
    "#print(list(segment))\n",
    "segs.append(\" \".join(segment))\n",
    "# print(segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 切分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x,y = zip(*sentences)\n",
    "train_data,test_data,train_target,test_target = train_test_split(x,y,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750, 6250, 3.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(test_data),len(train_data)/len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(train_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2 特征抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.1 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    ngram_range=(1,4),\n",
    "    max_features = 1000,\n",
    ")\n",
    "# 训练数据向量化\n",
    "X_train = vec.fit_transform(train_data)\n",
    "\n",
    "#测试数据向量化\n",
    "X_test = vec.transform(test_data)\n",
    "\n",
    "#预测数据向量化\n",
    "pre_data = vec.transform(segs)\n",
    "#print(X)\n",
    "word = vec.get_feature_names()\n",
    "# print(word)\n",
    "# def get_feature(x):\n",
    "#     vec.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf_X_train = transformer.fit_transform(X_train)\n",
    "tfidf_X_test = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 机器学习实现文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 使用词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "def calculate_result(actual,pred):\n",
    "    m_precision = precision_score(actual,pred,average='micro')\n",
    "    m_recall = recall_score(actual,pred,average='macro')\n",
    "    print('precision:{0:.3f}'.format(m_precision))\n",
    "    print('recall:{0:.3f}'.format(m_recall))\n",
    "    print('f1-socre:{0:.3f}'.format(f1_score(actual,pred,average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.781\n",
      "recall:0.781\n",
      "f1-socre:0.780\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train,train_target)\n",
    "# socre = classifier.score(X_test,test_target)\n",
    "# pred = classifier.predict(X_test)\n",
    "# test_target = np.array(test_target)\n",
    "# calculate_result(test_target,pred)\n",
    "classifier.predict(X_test)\n",
    "classifier.score(X_test,test_target)\n",
    "pred = classifier.predict(X_test)\n",
    "calculate_result(test_target,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 使用TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.784\n",
      "recall:0.785\n",
      "f1-socre:0.784\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(tfidf_X_train,train_target)\n",
    "classifier.score(tfidf_X_test,test_target)\n",
    "pred_2 = classifier.predict(tfidf_X_test)\n",
    "calculate_result(test_target,pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 预测属于类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tour'], \n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(pre_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 SVM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68655999999999995"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:0.687\n",
      "recall:0.827\n",
      "f1-socre:0.695\n"
     ]
    }
   ],
   "source": [
    "pred_3 = clf.predict(X_test)\n",
    "calculate_result(pred_3,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络实现文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.定义超参数，后面根据数据调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = tf.contrib.learn\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "\n",
    "MIN_WORD_FREQUENCE = 2\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "N_FILTERS = 10\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "FILTER_SHAPE1 = [WINDOW_SIZE,EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOW_SIZE,N_FILTERS]\n",
    "\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "\n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义cnn模型，两个隐层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(features,target):\n",
    "    \n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "                    features,vocab_size=n_words,embed_dim=EMBEDDING_SIZE,scope='words')\n",
    "    word_vectors = tf.expand_dims(word_vectors,3)\n",
    "    \n",
    "    with tf.variable_scope('CNN_Layer1'):\n",
    "        \n",
    "        conv1 = tf.contrib.layers.convolution2d(\n",
    "                word_vectors,N_FILTERS,FILTER_SHAPE1,padding='VALID')\n",
    "        \n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        \n",
    "        pool1 = tf.nn.max_pool(\n",
    "                conv1,\n",
    "                ksize=[1,POOLING_WINDOW,1,1],\n",
    "                strides = [1,POOLING_STRIDE,1,1],\n",
    "                padding='SAME')\n",
    "        \n",
    "        pool1 = tf.transpose(pool1,[0,1,3,2])\n",
    "        \n",
    "    with tf.variable_scope('CNN_Layer2'):\n",
    "        \n",
    "        conv2 = tf.contrib.layers.convolution2d(\n",
    "                pool1,N_FILTERS,FILTER_SHAPE2,padding='VALID')\n",
    "        \n",
    "        pool2 = tf.squeeze(tf.reduce_max(conv2,1),squeeze_dims=[1])\n",
    "        \n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(pool2,15,activation_fn=None)\n",
    "    loss = tf.losses.softmax_cross_entropy(target,logits)\n",
    "    \n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss,\n",
    "                tf.contrib.framework.get_global_step(),\n",
    "                optimizer='Adam',\n",
    "                learning_rate=0.01)\n",
    "    \n",
    "    return ({\n",
    "        'class':tf.argmax(logits,1),\n",
    "        'prob':tf.nn.softmax(logits)\n",
    "    },loss,train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:50281\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(\n",
    "                    MAX_DOCUMENT_LENGTH,min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words:%d'%n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.类别对应表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cate_dic = {'technology':1,'car':2,'entertainment':3,'military':4,'sports':5}\n",
    "train_target = map(lambda x:cate_dic[x],train_target)\n",
    "test_target = map(lambda x:cate_dic[x],test_target)\n",
    "y_train = pd.Series(train_target)\n",
    "y_test = pd.Series(test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.构造分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Mars\\AppData\\Local\\Temp\\tmptt32vsl4\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026B63EB2DA0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\Mars\\\\AppData\\\\Local\\\\Temp\\\\tmptt32vsl4'}\n"
     ]
    }
   ],
   "source": [
    "classifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.输出准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmptt32vsl4\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.70804, step = 1\n",
      "INFO:tensorflow:global_step/sec: 12.1625\n",
      "INFO:tensorflow:loss = 0.906659, step = 101 (8.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5506\n",
      "INFO:tensorflow:loss = 0.522719, step = 201 (7.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.967\n",
      "INFO:tensorflow:loss = 0.438215, step = 301 (7.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8374\n",
      "INFO:tensorflow:loss = 0.466484, step = 401 (7.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0419\n",
      "INFO:tensorflow:loss = 0.193651, step = 501 (7.668 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.267\n",
      "INFO:tensorflow:loss = 0.128035, step = 601 (7.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0368\n",
      "INFO:tensorflow:loss = 0.230962, step = 701 (7.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1172\n",
      "INFO:tensorflow:loss = 0.205118, step = 801 (7.623 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5734\n",
      "INFO:tensorflow:loss = 0.315877, step = 901 (7.953 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmptt32vsl4\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.261015.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Mars\\AppData\\Local\\Temp\\tmptt32vsl4\\model.ckpt-1000\n",
      "Accuracy: 0.890680\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(x_train, y_train, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = tf.contrib.learn\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 15\n",
    "MIN_WORD_FREQUENCE = 1\n",
    "EMBEDDING_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words:67631\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH,\n",
    "                                                        min_frequency=MIN_WORD_FREQUENCE)\n",
    "\n",
    "x_train = np.array(list(vocab_processor.fit_transform(train_data)))\n",
    "x_test = np.array(list(vocab_processor.transform(test_data)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words:%d'%n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words_model(features,target):\n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    features = encoders.bow_encoder(\n",
    "                features,\n",
    "                vocab_size=n_words,\n",
    "                embed_dim=EMBEDDING_SIZE)\n",
    "    logits = tf.contrib.layers.fully_connected(features,15,activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits,target)\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss,\n",
    "                tf.contrib.framework.get_global_step(),\n",
    "                optimizer = 'Adam',\n",
    "                learning_rate = 0.01)\n",
    "    \n",
    "    return ({\n",
    "        'class':tf.argmax(logits,1),\n",
    "        'prob':tf.nn.softmax(logits)\n",
    "    },loss,train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpi3ma6bzf\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AE0E0978D0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\Mars\\\\AppData\\\\Local\\\\Temp\\\\tmpi3ma6bzf'}\n",
      "WARNING:tensorflow:From <ipython-input-16-422ed1f48d70>:8: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpi3ma6bzf\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.70783, step = 1\n",
      "INFO:tensorflow:global_step/sec: 32.1219\n",
      "INFO:tensorflow:loss = 0.47365, step = 101 (3.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.6617\n",
      "INFO:tensorflow:loss = 0.580912, step = 201 (3.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.1578\n",
      "INFO:tensorflow:loss = 0.371338, step = 301 (3.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.2772\n",
      "INFO:tensorflow:loss = 0.311014, step = 401 (3.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.4564\n",
      "INFO:tensorflow:loss = 0.283106, step = 501 (3.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.835\n",
      "INFO:tensorflow:loss = 0.103793, step = 601 (3.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.0457\n",
      "INFO:tensorflow:loss = 0.150002, step = 701 (3.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9256\n",
      "INFO:tensorflow:loss = 0.146188, step = 801 (3.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 30.0966\n",
      "INFO:tensorflow:loss = 0.275054, step = 901 (3.323 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpi3ma6bzf\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.168855.\n",
      "WARNING:tensorflow:From <ipython-input-16-422ed1f48d70>:8: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpi3ma6bzf\\model.ckpt-1000\n",
      "Acc:0.890178\n"
     ]
    }
   ],
   "source": [
    "model_fn = bag_of_words_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "classifier.fit(x_train,y_train,steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test,y_predicted)\n",
    "print('Acc:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(features,target):\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "                    features,\n",
    "                    vocab_size=n_words,\n",
    "                    embed_dim=EMBEDDING_SIZE,\n",
    "                    scope='words')\n",
    "    word_list = tf.unstack(word_vectors,axis=1)\n",
    "    \n",
    "    cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "    \n",
    "    _,encoding = tf.contrib.rnn.static_rnn(cell,word_list,dtype=tf.float32)\n",
    "    \n",
    "    target = tf.one_hot(target,15,1,0)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(encoding,15,activation_fn=None)\n",
    "    loss = tf.contrib.losses.softmax_cross_entropy(logits,target)\n",
    "    \n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "                loss,\n",
    "                tf.contrib.framework.get_global_step(),\n",
    "                optimizer='Adam',\n",
    "                learning_rate=0.01)\n",
    "    \n",
    "    return ({\n",
    "        'class':tf.argmax(logits,1),\n",
    "        'prob':tf.nn.softmax(logits)\n",
    "    },loss,train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpnv3_stbn\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000026B656F05C0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': 'C:\\\\Users\\\\Mars\\\\AppData\\\\Local\\\\Temp\\\\tmpnv3_stbn'}\n",
      "WARNING:tensorflow:From <ipython-input-18-f5b552450cff>:16: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpnv3_stbn\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.70789, step = 1\n",
      "INFO:tensorflow:global_step/sec: 20.1457\n",
      "INFO:tensorflow:loss = 0.416362, step = 101 (4.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.4969\n",
      "INFO:tensorflow:loss = 0.496586, step = 201 (5.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5833\n",
      "INFO:tensorflow:loss = 0.377564, step = 301 (5.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5737\n",
      "INFO:tensorflow:loss = 0.330479, step = 401 (5.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5221\n",
      "INFO:tensorflow:loss = 0.208163, step = 501 (5.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.4783\n",
      "INFO:tensorflow:loss = 0.117927, step = 601 (5.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.6623\n",
      "INFO:tensorflow:loss = 0.19039, step = 701 (5.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.8477\n",
      "INFO:tensorflow:loss = 0.126967, step = 801 (4.795 sec)\n",
      "INFO:tensorflow:global_step/sec: 20.3634\n",
      "INFO:tensorflow:loss = 0.17885, step = 901 (4.911 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpnv3_stbn\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.160479.\n",
      "WARNING:tensorflow:From <ipython-input-18-f5b552450cff>:16: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From D:\\tools\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.framework.python.ops.arg_scope) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Mars\\AppData\\Local\\Temp\\tmpnv3_stbn\\model.ckpt-1000\n",
      "Acc:0.892324\n"
     ]
    }
   ],
   "source": [
    "model_fn = rnn_model\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=model_fn))\n",
    "\n",
    "classifier.fit(x_train,y_train,steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test,y_predicted)\n",
    "print('Acc:{0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
